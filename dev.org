#+TITLE: Infograph
#+STARTUP:nologdone

* Intention
** Summary
	 The goals of this project are hard to summarise because they're continually
	 growing and changing, overreaching all reasonable scope and being pruned back
	 again.

	 The core idea is a new way to create UIs by *drawing* them. It's not a visual
	 programming language in the normal sense of visually laying out the control,
	 or data flow, but rather the desire is to be able to create a fully
	 functional spa with what are normally considered "design tools".

	 This is first and foremost an experiment. An experiment in UI design, AI, and
	 HCI. I will take no assumptions for granted and intentionally defy existing
	 norms whereever I find them to be counterproductive, confusing, or just not
	 especially helpful.
** Examples That Should Work Out of the Box
*** Heat map
		Array of values, or objects with a single value, x and y coords are
		functions of index and colour function of value.

		Or array of objects with x, y, v props.

		Or matrix representation of single values.
*** Pie Chart
*** Histogram
*** Graphs (i.e. plots)
*** Graphs (i.e. network graphs)
*** Mandelbrot Set
		Or other fractal patterns
*** Lazy loading of infinite data
		Presumably via AJAX

* Work
** Unsorted Ideas
	 Ideas that I've jotted down, but haven't thought through and inserted into
	 the backlog yet.
*** Very Lazy Rendering
		When dealing with on screen logic, we should be able to filter the vo model
		so that only shapes that actually appear in the current window even need to
		be processed for events, properties, &c.. This is technically an
		optimisation, but if things start to grind to a halt, we know why.
*** New colour scheme for data and properties
*** Drag points to points
		Maybe we should be able to drag the points at the edges of lines into one
		another to force them to be the same.
*** Infer desired property
		From the type of data we might be able to guess what it's being dragged
		to. If we can and the user drops immediately just do it.

		I think this is redundant with an idea somewhere else.
*** Prettier property windows
		Not a priority for the proof of concept, but boy are they ugly.
** Bugs
*** Origin isn't on default canvas
		The canvas thinks that (0,0) is the bottom left corner, but that corner is
		actually some 20 or 30 pixels beneath the browser window.

		It's a canvas sizing issue.
*** Type checks on dropped properties
		If I drop an int into a property that knows it should be a coordinate, that
		should cause an error.

		This will probably get fixed along with contextual highlighting of the
		subtables.
** Next Ideas
	 Roster of all ideas I've had regarding this project. Roughly sorted by
	 importance in a purely subjective fashion.

*** Infinite Images
		Images should be able to span the entirety of Euclidean space since only
		that part that fits into the current window needs to be rendered.

		Particularly useful here would be axes or grid lines.

		To accomplish this, I would need both an encoding for things of infinite
		length — for grid lines I can use +/-infinity, but for an infinite diagonal,
		we need a different represntation — and another layer of indirection — or
		maybe just an addition to the projection step — to cast these infinite data
		structures into things that won't crash the browser.
*** Inifinite detail
		Images should be able to contain subimages of arbitrary complexity and
		depth. This will allow the rendering of fractals and recursive patterns. In
		principle there's nothing preventing it since only finitely many things can
		appear on the screen (there are only finitely many pixels).

		Using a lazy rendering approach images smaller than a certain size don't
		need to render subimages.

		Much more difficult is the scenario where an infinite number of images cover
		the screen. After some point there will just be black pixels everywhere and
		the rendering can stop, but how can we tell?

		Is the second aspect useful? The first certainly is.
**** Uses
		 - Mandlebrot set
		 - Hopf bifurcations
		 - Escherian infinite regress tiling
		 - Anything not a fractal?
*** Constraint Based Properties
		It should be possible to specify properties as functions of other
		properties. e.g. enforce a square is square by setting h = w, calculating
		the endpoints of a line to fall on two dynamic circles, etc..

		But we are also going to need genuine constraint based programming for
		things like making sure circles can't overlap, that the line connecting two
		circles is as short as possible. and so on.

		That's not so hard from a CS point of view, but I haven't looked at this
		stuff since undergrad, so it's going to take a good chunk of work.
*** Moving the Canvas
		- Note taken on [2017-08-04 Fri 10:17] \\
			Panning currently works as desired. Zoom works, but the cursor doesn't
			stay centred when zooming as desired. This is just because I haven't sat
			down and done the math. Not my top priority though at the moment.
		Draging and zooming should move the canvas around as if it were a map.

		Interactive infographics (those not exported to pngs, gifs, etc.) should be
		able to pan and zoom as well if the author desires since this can allow
		exploration of far more data than is possibile in a fixed size. Lazy
		rendering can allow images to be effectively infinite in extent, detail, or
		both. I feel this will open up whole worlds of pedagogical value.

		Trivial examples: the Mandlebrot set. or any fractal. All of Facebook as a
		friend graph (that would require more than a little data magic but I'm sure
		somebody will get a kick out of making a library for it).
*** Drag hover highlighting in property windows
		We need to indicate clearly what will be effected on drop. Highlighting the
		candidate to be replaced seems like a good solution.
*** Unified Ambiguous Shapes
		Shapes can be represented in many ways. Don't choose one and enforce that on
		the user. Learn to deal with diversity and ambiguity (ambiguity because you
		don't always know what circle means, you know a bunch of things circle could
		mean and there's always the chance they're all wrong.

		This is critical, but I'm still not sure how to do it, so it's not at the
		top of the list.
*** Graphical Adjustments
		Every aspect of a shape you see should be adjustable in place just like
		illustrator et al.. drag control points on bezier curves, drag lengths,
		click centres to choose colour, gradient, etc.. The stroke types themselves
		should be adjustable as such, but I'm not sure how to accomplish that
		graphically.

		Dragging a data value into a point, line, etc. should link the data to the
		property in question. The problem of course is that pointing is ambiguous
		and easily misinterpreted. Data parsing and typing will solve some of the
		problems: colours set colour, strings set string properties, lengths must be
		numbers, pairs of numbers might be coordinates. But the ambiguity is still
		there: a number dragged onto a line might be the length, its width, it might
		be the angle to the origin, it might be the x or y translation, it might be
		a label (but not very likely), so how do we disambiguate in an intuitive
		manner?

		Each shape will have a list of properties, so if you drag the data over and
		wait a split second, a little property list can pop up and you drag the data
		to the correct one. This will be especially useful when the x and y
		coordinates of a point are different paths in the data for instance. If by
		the nature of the data and the properties we can make an educated guess as
		to what the user means, then we should order the properties accordingly and
		if the user just quickly drops the data without waiting for the menu then
		bind it to the first entry (most likely guess).
*** Code Adjustments
		Sometimes you want more precision than a mouse offers. Or you have exact
		mockups (though this is supposed to remove the need for mockups). For this
		you should be able to open the property list of an object and manually
		change the values. Handy would be editing widgets a la Victor's magical
		editor from the talk on principles. Would those actually be handy or just
		candy?

		This will likely also be the only viable way to set constraints between
		properties and data or to use compound data. E.g. you want a rectangle whose
		width is always half its length which comes directly from data, or you want
		a circle whose radius is some function of the number of other objects that
		match a criterion.
*** Higher Order Data
		We will often want to drive visual objects from properties of the data in
		aggregate (averages, percentages, number of categories, etc.) or functions
		of the data (number of Xs OR Ys, people with more than N friends who voted
		for a given party (this one is actually a query, but we can handle that
		right? Spectre is a great candidate although I feel that it will need some
		massaging so as not to scare away visual artists, journalist, teachers, and
		the rest of humanity that I want to feel comfortable using this.

		This should be pluggable in a simple way so that people can share
		aggregators, post-processors, etc.. Library design? Read jars from clojars
		from the client and hotload cljs? That would require users to post to
		clojars and I'm not sure that's realistic. Post stuff to clojars via ajax
		from the browser? New repo site and mechanism? Sharing should be as simple
		as tweeting, but finding code shouldn't be as hard as searching twitter...
*** Dynamic Data Visualisation
		Given example data (or a schema or spec) the program should parse that into
		a nice little tree and try to guess the correct types of the fields and
		provide a drag and drop interface from the data into the visual field. This
		is much easier than the next point since it only goes one way. The data is
		give and the image is generated from it. Just like a frame in react.
*** Testability
		I want these infographics to be truly dynamic. That means that you have an
		example of the kind of data you're going to get, but in the real world you
		don't know what the data is going to be until you get it. You should be able
		to design an election map with random data and see it populate in real time
		as you update the data from the polls.

		Problem: what if your data is structurally incorrect?

		Well the best we can do in this case is spot that there's going to be a
		problem before we render complete nonsense and apologise profusely to the
		user. This is hardly ideal. Perhaps we can specify fallback strategies,
		allow properties to be marked as optional. But then how do we assure
		ourselves that the graphic will still make sense when fields are omitted?

		Problem: what if the data is structurally correct, but the example data is
		unrepresentative in some way? E.g. all of the values for x in the example
		set are between 0 and 300, but in reality they go up over 9000.

		Again the best we can do after the fact is simply catch absurdities and
		error out with something approaching grace. Until we have 100M displays at
		least.

		As far as I can see at the moment the only way to deal with these problems
		is to try and prevent them. If we have specs for the visual objects, and we
		can generate (or are given) specs for the data, then we can generate example
		datasets, create graphics from them, and show the designer a (very) long
		list of example graphics.

		Basically generate data and pipe it through the pure rendering functions and
		make the designer okay things.

		Binary search over N properties will get tedious so I doubt people will do
		much of it. If we can find a way to specify property constraints (things
		can't move off the screen, no circle should have a radius greater than the
		width of the screen, the number of objects should be less than 1/10th the
		number of pixels, &c.) then we can massively narrow down the number of
		examples the designer needs to vet and even do a bunch of testing fully
		automatically. Sort of. We still run into problems if the data that comes
		from the real world doesn't match the model that the graphic was based on,
		but when is that ever not a problem?

		So in short we should strive to protect the user against mistakes from
		narrowness of vision, just poke them to think "oh yeah, that might be
		negative", but there are always assumptions at the bottom.

		So what if the renderer can learn to improvise? Then I get rich?
*** Shape UUIDs
		Right now compound shapes contain a set of subshapes. Shapes are then
		references to themselves in this bag. With immutability this seems good
		enough for now, but something keeps nagging atthe back of my mind telling me
		to replace this by maps from uuids to shapes. Is this just my imperatively
		trained past sneaking up on me? If we use UUIDs then wouldn't we need to
		change the UUID every time the shape changes? And then we'd be exactly where
		we are with sets, no?

		I need to sort this out one way or another.
*** Leaning from aggregate use
		Wouldn't it be swell if the platform could detect that you're defining
		something equivalent, or nearly so, to something that it's seen before and
		present you with the other option? This would both aid discovery and help
		reduce the number of reinvented wheels. The duplication of effort, and the 6
		80% done libraries doing X (for all X) that is both lisp's blessing and its
		curse could perhaps be mitigated. Genuinely new ideas can be traced back to
		the things they evolved from, and rediscoveries can be merged or pruned away
		over time.
*** Duality of Drawing Code
		The code that draws the shapes the user sees should be open to inspection
		and modification. This is a whole other can of worms that I'd rather not
		open at the moment but I do want to pursue this. Likely as next project.
*** Allow visual objects to respond to user input
		It should be possible to specify properties of vos that respond to the user
		clicking, mousing around, etc..
** Current
*** Time / Space modes
		Do you want to represent multiple data points as an animation, or as somehow
		arranged in space? Of course you can do both simultaneously with difference
		dimensions of the data.

		As a consequence, animations have to be first class. I don't yet know what
		that entails, but it doesn't sound simple... Frame rate? Tweening? Layers?
** Done
*** Property windows for shapes
		I've been so tied up figuring out canvas that I haven't actually done
		anything regarding dragging data into shapes. There's a lot of work to be
		done before the core value of the app exists.

		Every shape should determine (a series of) tables which lay out its
		properties.

		These are again tied up with ambiguity in shapes. They need to be able to
		represent a point as a pair or as individual coordinates, they need to know
		that lines have length, and endpoints, but that they aren't
		independent. Most importantly these classes need to be open to extension by
		the designer at runtime.

		Still need to think up a representation for data bound properties. Otherwise
		this has proven its concept.
*** Drag data to properties
		Data fields dragged into properties should set Schema values on those
		properties.

		There also needs to be a way to set a computation based on a data value.
*** Pixel distances
		The current Euclidean distances are nice mathematically, but as we zoom the
		image they make the distances vary in a non intuitive way for the user. If I
		drag something a pixel from something else, I mean that thing, It shouldn't
		matter if that pixel corresponds to a distance of 1000, or 0.001.

		Bumped up because this will actually simplify a lot of the geometry logic.

		Working now in a basic way, but doesn't play along with moving the canvas.

		I was projecting the drag position...
*** Clean up this file
		So much rambling thought process. Need to go through the whole thing and
		consolidate.
*** Event inversion
		Need to inversely project events into the cartesian plane so that clicking,
		dragging, etc., act on the things being clicked, dragged, etc..

*** Explicit origin
		The origin of the canvas should be a point in space, not the upper left
		corner. It should also default to the lower left corner so as to infuriate
		CG veterans. Do we want axes or is that just silly? I want axes. Is that
		sufficient reason? Maybe even a full grid.
*** Basic vector drawing widgets
		Line, square, bezier cubic, circle, elipse, etc..

		Free drawing should be allowed but we need some kind of constraint
		satisfaction engine before we can force a free drawing to adhere to changing
		data.
*** Simple Model of User Interaction
		- Note taken on [2017-08-04 Fri 10:49] \\
			User interactions are modeled identically to designer interactions. I think the
			basic idea of how to do this is well established, but I don't yet know what to
			do with it.
		Steal a note from Elm and model user interation as just another data
		stream. How far can I take this? Obviously what I want is a programming
		environment that makes handling users clicking, draging, mousing around,
		just the same as it handles drawing a rectangle and making its length
		correspond to some property in data that you'll get from a server at
		runtime.

		Will I get there? Who knows? but that's no reason not to try.
* Notes
** Shapes
	 The representation of shapes is actually a bit tricky. Mostly because there
	 are so many ways to think about shapes and I can think of interesting ways to
	 tie each representation to data in new ways. Think of what polar coordinates
	 make trivial that's absurd in cartesian coordinates. Think of what you can do
	 by setting the end points of lines, and now think of what you can do by
	 setting the centre point, length, and angle independently. In the second
	 representation you have 4 legitamately independent variables that can show 4
	 dimensions of data. In the first you have error bars, and other things that
	 are really 2 independent 2 dimensional variables. That doesn't sound like
	 much, but the difference is real.
*** Representations of the Circle
		A circle is in many ways the simplest shape. All representations are
		isomorphic to a centre point and radius. Well that's not saying much. All
		representations of the same shape are isomorphic by definition.

		An affine transformation of a circle scales it (changes the radius) and
		translates it. So the connection is obvious.

		You can represent a circle by a point and a diametre vector, as per
		Euclidean algebra. Again the transformation between the two coordinate
		systems is obvious.
*** A Rectangle is More Interesting
		Width, height, lower left corner

		Lower left and upper right corners

		Lower right and upper left corners

		Ratio of width to height and affine transform (takes unit square at the
		origin to this rectangle).

		Diagonal line (either one)
** Transformations and Equivalence Classes
	 Needless to say, there are lots of ways to represent even simple shapes. The
	 number of possible representations of complex shapes goes to
	 inifinity. Particularly when you consider that there are exponentially many
	 ways to break down a complex shape into parts.

	 The core drawing language can't hope to have all of these different
	 breakdowns built in. I'm skeptical that it's even possible to specify them
	 all in a closed form kind of way (of course simple geometric figures are just
	 group actions, but the breakdown of complex figures isn't treatable
	 algebraically to my knowledge).

	 In any case, including all possible breakdowns would defeat the goal of a
	 simple, cohesive core language. Shapes should have intuitive, general
	 cannonical representations, and an intuitive extension system. I'm directly
	 contradicting myself here about cannonical reps, so there's a more subtle
	 point here: the extensions can't be second class to the built-ins. Extension
	 of the language has to be first class. Ideally the extension happens in the
	 graphical editor just like creating infographics. Draw two rectangles and
	 drag properties (through arithmetic operators) of one to the other.

	 One options would be to include a full ontology of every possible property of
	 all built in objects, but that precludes our first principle: the language
	 should be extendable by the user in any direction, especially those we
	 haven't thought of yet.

	 So I think I just need to pick a representation and use some kind of logic
	 programming or search to sort through transformations until one is found that
	 makes the data match the spec. That could be computationally intractable. But
	 let's worry about that later. Maybe specs aren't a good choice, for that
	 reason.
** Computed Properties
*** Early Ideas
		So how do we go about linking data to the properties of visual objects?

		Internally the visual object will be represented as a map of properties. We
		could set the values of those properties to reaction like functions. Or we
		could set them to atoms that would need to updated elsewhere (bad idea), or
		we could represent the shape itself as a stream with each instance being a
		concrete, renderable entity.

		How do we want to compose these dynamic shapes? Should a composite picture
		update atomically, or should the subobjects update only when needed. The
		waste of rerendering on every frame would be huge, so let's not do that. I
		think we need to steal the lazy rendering model from somebody (reagent, om,
		react itself?). At least we don't need the virtual dom.

		So what if we have functions like

		#+BEGIN_SRC clojure
		 (link-property {} :length length-from-data)

		 (-> shape
				 (link-property :length f)
				 (link-property :x g)
				 (link-prpoperty :y h))
		#+END_SRC

		That's not very nice.

		Values coming from a data set will have an implicit path, so we could do
		something more like:

		#+BEGIN_SRC clojure
		 {:type               :linked-shape
			:base-shape         {:type  :line
													 :style {:colour "#FF0000"}}
			:dynamic-properties {:length {:path [:a ANY :l] :tx f}
													 :x      {:path [:a ANY :x]}
													 :y      {:path [...]}}}
		#+END_SRC

		But then why not represent the shape itself as

		#+BEGIN_SRC clojure
		 {:length (->DynamicProp [:a ANY :l] :tx f)
			:x 34
			:y 75}
		#+END_SRC

		I don't like these spectre like ANYs lying around. Do I need a path query
		language or should I prefer a recursive design? What would a recursive design
		look like?

		Walk the input data tree and at each object find an appropriate parser and
		parse it, if a property contains an object recurse, if it contains an array
		of objects recurse (what if it contains an array of values?). Can this
		capture enough context to draw what the user wants?
*** Current Idea
		So for the time being I've created a pair of types ValueSchema and
		ShapeSchema that allow the contruction of shapes as values even though the
		values aren't defined apriori. The new types plus built in types implement
		an `Instantiable` protocol which, given data, does what you'd expect.

		Down side: This assumes a tree structure to the incoming data. Given my
		original use case of building a graphic from a json file of API, this is
		fine, but real data is linked in complex ways and this approach is
		fundamenttally limited.

		The Schema types take a "query" which is at the present just a vector of
		keys to be passed to `get-in`. This is the arboreal bottleneck. I don't see
		why we couldn't replace the vector paths with datascript queries in
		principle. Maybe I'm not as locked in as I first thought.

		Either way getting something basic working is priority one, so let's not get
		lost in the cave.
** Graphic Design Model
*** Previous Thoughts
		- Note taken on [2017-07-26 Wed 10:50] \\
			I'm keeping around outdated theorising so that I can track my thought
			process over time. Will keeping this stuff around be too confusing? Will I
			actually go back and benefit from seeing my past mistakes again? Sounds
			like an obvious yes, but I'm suspicious.

		So we have two fundamentally different sources of data. We have domain data,
		that is the JSON, or whatever comes in that will ultimately generate the
		graphic. This is comparable to a compile time thing. The second kind of data
		is the user's interaction history. These determine the state of interactivly
		defined widgets such as shape constructors.

		Thus we have a calculus with two operators: instantiate and react — names may
		vary. instantiate takes shape schemata to shapes. react takes shape templates
		to shapes. The two operators are idempotent and commute. That's a nice simple
		algebra. There's probably a whole theory of things like that if only I knew
		the name.

		See simple category diagram in notebook. I don't want to copy it at the
		moment.

		Widgets, unlike shapes have a lifecycle. Constructors in particular have to
		remove tHemselves and add concrete shapes in their place. Really this amounts
		to reacting in the source data itself. A partial evaluation of the data
		Template.

		This is the opposite of data linking, where concrete shapes need to be
		replaced with shape schemata.

		Generalisation and specialisation. Familiar theme?
*** Current
		There are different kinds of data, but in reality they all behave the same
		way as far as visual objects are concerned. All the objects care about is
		getting data, they have no notion of origins.

		The sources of data themselves are quite different in content, purpose, and
		origin, but I think I can fully insulate the shapes from those details.

		We still have the interesting phenomenon of deinstantiation that needs more
		thought.

		There's a fundamental symmetry between instantiation and
		projection. Wouldn't it be facinating if there were a simmilar symmetry
		between deinstantiation and coprojection?

**** Partial Instantiation
		 Given the lifecycle we have where some data is completely static, some is
		 static by the end of development, and some is undetermined until the very
		 last minute, there might be real performance gains to be had by partially
		 instantiating the data and then only instantiating the rest when the time
		 comes. Especially if instantiation starts to involve actual querying. That
		 would be fairly trivial to implement if we take the convention that
		 Schemata can't be instantiated by nil and just return themselves when that
		 is attempted.

		 Let's not overthink this at the moment, but it's good to know that we have
		 a way to go if it becomes necessary.

** User input
	 Touches and mouse movements need to be resolved into paths whose state is
	 tracked in the app.

	 This is the only way I can see to resolve multiple simultaneous touches into
	 separate drawings.

	 Also if each path has a unique id then a shape constructor can be bound to
	 the head of that named path and listen for that path ending to reify
	 itself. I need a more fine grained vocabulary.

*** Plan
		Stratified design.

		Level 1 aggregates all mouse events into some sort of indexed data
		structure.

		Level 2 maps aggregated events into higher level constructs that we care
		about.

		Level 3 reduces over the higher level events to produce the app state.

		This kind of separation will allow me to focus on single touch for now with
		minimal changes to convert to multi-touch. Multi user would be another layer
		inserted between 2 and 3. Eternal conundrum: Put in a dummy layer now, or
		just accept that I'll need to refactor. Knuth wins.

** Interactive Canvas
*** Outdated Ideas
**** Ramblings (Don't like this approach)
		 So we don't have an event model on canvas. I knew that, but I've been putting
		 off thinking about it.

		 Basically we want something finite state machine like. Given the JS event
		 model, we may as well use continuations for control. Clean it up with
		 core.async though.

		 So a click or a dragstart or a hover will create a new continuation which
		 will listen on some sort of pub-sub setup for whatever kinds of events it's
		 interested. It will emit new state as a side effect into the app-db — which
		 will be a likely source of trouble — and will eventually teminate. So think
		 of the canvas handler as an actor factory where the actors are always short
		 lived. So coroutines. But not quite since they get messages over async
		 channels. I'm sure this wheel has been invented before.

		 Anywho, that seems reasonable. It will allow multiple of these things to be
		 running at once so that designs can react in parallel to user input.

		 There's going to be trouble with reloading since the current user action will
		 be spread out through async oblivion. I suppose I can solve this with an elm
		 like approach where the state of this mess of continuations is a function of
		 a stream of events. So I can save the user inputs, clear the system, restart
		 it, and play them back. Checkpointing will be pretty easy since once an actor
		 exits, the events it consumed are persistent in the app state, so if no
		 living actor has cared about an event we can drop it.

		 State management is also going to be a problem. The user selects the line
		 widget, then clicks, drags, and releases on the canvas. We need this event to
		 add a line from click start to click end to the drawing state. Dragging a
		 value from the data into the canvas should trigger popups when the drag
		 pauses over a visual object. Dropping into a visual object before the popup
		 appears needs to directly update the state. And so on.

		 All of the above is easily enough done, but we need to retain enough
		 tracability and transparency that we don't end up in a tar pit. That sounds
		 exciting.

		 Lots of exciting edge cases to worry about. Like the fact that every visual
		 object currently on the screen needs to listen for hover and dragover
		 events. These guys are in fact going to be more or less permanent.

		 Q: How do we tie the set of actors corresponding to the shape to the shape
		 itself? Well actors and objects are heavily related, so why don't we have the
		 canvas state be a set of records. One for each visual object and one for the
		 canvas itself (we want to be able to zoom and pan the canvas itself, and we
		 need something to listen to clicks and create the constructor objects
		 (special temporary visual objects that exist only to provide visual feedback
		 when making new visual objects.
**** Questions
		 Should all visual objects be children of the canvas object? Should visual
		 objects have children on their own? It makes perfect sense to represent a
		 rectangle as a line, or as four lines with constraints, but are we trapping
		 ourselves by allowing these relations to be reified in the object model? I
		 think so. My thinking at the moment is that the visual objects should be
		 those things the user explicitely creates. Might it make sense to allow
		 agglomerates? Might it then make sense to have equivalences and dynamic
		 tansformations between equivalents? How would I do that?

		 So what do we do about multiple event listeners and bubbling? I don't know
		 much about these things. I might be getting out of my depth...
**** Details
		 So we have 3 kinds of visual objects.

		 1. The canvas itself
		 2. Constructor objects
		 3. Normal visual objects

		 These can be implemented as records. The UI can extract the draw info out of
		 the records. The records themselves could be responsible for instantiating
		 the drawings with the data. That needs a bit more thought. These records can
		 implement protocols for handling user input. Then we can have a central
		 pub-sub system that notifies all records that implement a given protocol
		 when the associated events come in. I don't see a down side here just yet.

		 The amount of computation involved in a user click could easily grow out of
		 hand as the number of objects subscribing grows. The objects need a
		 knowledge of themselves in space so that they can cheaply decide if an event
		 concerns them. Moreover this proprioception should be exposed so that if it
		 comes to it a higher level dispatch agent can more efficiently decide to
		 whom to send which events. That's an optimisation that we can ignore
		 temporarily.

		 N.B.: Run a test on this as soon as we can to make sure that scaling isn't
		 completely attrocious. We want this to be usable.

**** Ways to accomplish this
		 This is getting difficult. Who'd have thought that designing what amounts to
		 a ui building ui more or less from scratch would take a lot of thought?

		 We have a number of options to represent visual objects. They could be
		 independent actors, they could be nested so that the canvas can be asked for
		 the draw state of everything.

		 So if we make shape schemata functions that take data and return shapes,
		 have specs for shapes, then we should be able to generate said functions
		 pretty trivially.

		 Composition is a problem though. If you have a function that returns a line,
		 and then a compound schema which is a function that returns a compound
		 shape, you can't add another line to the compound schema since it's a
		 function. You rather need to look at what made the function and then add a
		 line to that and then make a new function. In otherwords we need a data
		 representation and generic instantiation.

***** Considerations
			- Event bubbling
				This has always seemed to me like a sore spot of js and the dom, but we
				do need someway to decide which of several overlapping objects gets a
				message.

			- Extracting the draw state
				However we implement visual objects the state of the canvas has to be
				directly accessible with minimal coordination.

			- Changing values over time
				Shapes are immutable. Transformation functions (like affine txs) return
				new shapes.

				Visual objects have shapes, but they are much more complicated. If we
				store shapes in the visual objects then we have a mutation problem: what
				does it mean to grab a widget and drag it to resize a shape? does the
				visual object contain a reference to a shape changing over time? That's
				one way to do it, but then each object has to keep an undo history and
				something global has to order them so that undo and redo operate sanely.

				We could alternately model the canvas as a single immutable composite
				shape. Use an event sourcing model like Elm's to take Event -> Shape ->
				Shape which would give us a nice functional feel and easy
				undo(tree!). Visual objects then would not really be objects, but event
				transformers that take a Shape -> DOMEvent -> Maybe Event.

				We can use re-frame's event handling logic to handle this. We can also
				just keep a list of previous states for undo purposes. Undo granularity
				is an eternal problem, but we can figure it out.

			- Time travel debugging
				This sounds like it would be useful, but I'm not sure it would
				be. Certainly not until the end user starts to create interactive things.

			- Backtracking / Undo / Redo / Undotree
				I'm far from certain that undotree is a useful — read useful as can and
				will be used — feature. I saw a really cool gui undotree style browser
				history navigation widget in a paper once. That could conceivably bridge
				the gap.

				In any case we want effectively unlimited undo/redo.

			- Collaborative editing
				It doesn't have to be multi-client, it could just be a bunch of fingers
				on a single touch screen, either way it should play nice.

				So how would we handle multi-user simultaneous editing? We can't totally
				order events, but if we can model shapes as a CRDT then we'd be on
				comparatively easy avenue.

			- Instatiation
				This is a tricky one. We need two layers of representation for this. We
				need data bound shapes, which without data can't be drawn (unless we fake
				data, which might be useful for protoyping), and we need the concrete
				shapes. A change in the data will generate a new canvas.

				If we bind a given piece of user data to a single compound shape, then
				composition becomes simple (look at this data vis and that data vis side
				by side? No problem). Similarly animations just become streams of shapes
				with a framerate.

				So what's the best way to represent data bound shapes? Maybe something
				like Data -> Events -> List (AbstractShape | ShapeConstructor) -> Shape?

				So does this mean that we want shapes to be a subscription from
				shapeconstructors / abstract shapes? That would give us the simplest
				reactivity...

				Keep in mind that we want one unified representation to deal with shapes,
				abstract shapes, and shape constructors. Abstract shapes depend on the
				data, shape constructors depend on user input, and shapes are just data.

** Levels of Abstraction of Canvas
	 So we have a stateless wrapper over canvas. This is just a convenience to not
	 have to manually manage global state.

	 Over that we have what I'm calling a window. This is a model of 2d space as
	 the (infinite) cartesian plane with a window showing some finite rectangle of
	 it at a given projection factor (linear projection). Note that this window
	 also converts the cartesian y axis to the cg y-axis so that from a user's
	 point of view coordinates work like they ought to.

	 Over this — conceptually, though in implementation the same object might
	 satisfy both protocols — we have an event handling system that does the
	 inverse translation from pixel coordinates to cartesian coordinates and
	 delegates events appropriately to the things effected.

	 Within the window we have shapes, or visual objects, which are dynamically
	 dependent on data external to themselves. The nature of this data falls into
	 4 categories:

	 1. Static
			Constants, frames, backgrounds, other fixed things.
	 2. External API
			JSON, or whatever containing the info of the graphic.
	 3. Designer input
			That is user input during the graphic design phase. In this way,
			interactive component creation mimicks use.
	 4. User input
			For interactive designs.

	 Notice that each of these data types can be applied independently and except
	 for the last ahead of time. This allows us to "compile in" the data we want
	 to use to deliver an infographic as a single js file. Not sure that's useful,
	 but there's a nice mathematical feel to the commutative idempotency.

** Cartesian and Pixel Windows
	 Really a window is two polymorphic functions:

	 project :: W -> ℜⁿ -> Pⁿ, ∀ n
	 inject :: W -> Pⁿ -> ℜⁿ, ∀ n

	 project w inject w p = p
	 inject w project w q = q

	 Plus a monomorphic function:

	 contains? :: W -> ℜ² -> Bool or W -> P² -> Bool

	 I'm not sure which is more appropriate. Maybe we want both.

	 N.B.: Above ℜⁿ are n-dimensional real coordinates and Pⁿ are n-dimensional
	 pixel coordinates. W is a window. The dimensons of coordinates need to be
	 strictly smaller than the dimensions of the window for the functions to make
	 any sense.

	 So what's a window? A (2D) window is two rectangles, one in pixel space and
	 one in real space where the ratio of width to height is identical. The
	 location of the rectangle is always fixed with the top left corner at the
	 origin. The rectangle in real space is free to move anywhere it pleases. Note
	 that the y-axis is flipped in pixel space but *not* in real space.

	 The ratio of \frac{h_ℜ}{h_P} is called the zoom. This ratio is the same as
	 \frac{w_ℜ}{w_P} since the aspect ratios of the rectangles are assumed to be
	 equal.

	 Thus the window is uniquely determined by a corner in real space, the width
	 and height in either space, and the zoom factor.

*** Concerns
**** Zoom
		 I still need a good way to turn a bunch of ints into a real between 0 and infinty.
		 The reduction over the ints needs to be associative and commutative otherwise
		 behaviour is going to be weird.

		 What if we just take the sum and then apply 1/-x to negatives (1, 0, and -1
		 are treated as 1). Let's give it a shot.

** Shapes, Visual object, and Drawing
	 - Note taken on [2017-07-25 Tue 11:24] \\
		 These notes are in the order I thought of them, That means in general that
		 the earlier ones are outdated and the later bits are redundant. The themes
		 that recur over and over are the ones that end up getting implemented. I
		 read something where Andrew Bird said that that's how he composed music. I
		 wonder if that's a good excuse?

	 Looking at the instatiable and projectable protocol implementations, they're
	 identical and contain no information. Given how Instantiable is implemented
	 for the built in types, that's kind of silly; we don't need to implement
	 Instantiable at all for the different shapes because it does the right thing
	 as implemented for maps.

	 The deal with Projectable is the same. We just need a base type for 2D
	 coordinates so that we can project them correctly. All numbers should be
	 treated as scalars — I don't see any context in which we want a number to
	 stay fixed as we scale the entire graphic — so we don't need another type for
	 that.

	 With these changes, shapes just become data again. I think we still want them
	 in records because draw is going to have to be polymorphic in the shape. What
	 else is going to know how to draw something? A third party is what. I want
	 ambiguous shapes. That is multiple representations of the same thing, not all
	 of which need to be compatible. If we have two distinct ways to draw a line,
	 you can either map your line to one of those, or define a new way to draw a
	 line from your data. This is going to cause a proliferation problem unless
	 the runtime is able to realise that you're reinventing the wheel and help
	 correct you. That said this is one of the great problems of programming and
	 I'm not really expecting to solve it...

	 So what if we just use maps, and then have a multimethod dispatch on the
	 result of a search for an appropriate renderer? That sounds expensive. But
	 that's no reason not to try.

	 I don't think we want to trap data in records. It makes instantiate and
	 project a real pain, but more importantly it it going to make ambiguity much
	 harder to handle.
* Log
** Sketchpad D3
	 Graphical interactive interactive infographic creation.

	 Thinking about this as I walked to the market I got pretty excited. This is
	 such a simple demo of the basic idea with a clear and obvious use case for
	 lots and lots of people.

	 Two panels: code on the left and a drawing tool on the right. Don't start
	 with free drawing, it's too messy and people won't use it for a lot of
	 things. We can get really far with line, rectangle, circle and ellipse.

	 Data will have a structure, so drag properties from your data (presumably the
	 objects) to the properties of the things you drew (lengths, colours, labels,
	 angles, whatever) to connect them. Now you can instantiate multiple objects
	 from you drawn prototype. Of course we need some kind of validation on the
	 incoming data, but we can probably generate specs given specs on the shapes
	 and the user entered connections!

	 Arrays of things either represent a sequence in time, a sequence in space, or
	 a bag that should be a set but is a vector because people just do that all
	 too often. Or it's a struct with implicitely ordered fields, but let's
	 pretend we never thought of that.

	 So say the user has a vector of things. They can choose to either treat those
	 things as a sequence in time (frames in an animation, for instance). or as
	 things arranged in space. That arrangement in space is infinitely flexible
	 and up to the user's skills as an artist. Basic examples would be a
	 histogram, a pie chart, widgets for countries superimposed on a map, cells in
	 the game of life, a link analysis graph. The link graph is interesting
	 because those are notoriously finnicky, and we are going to need some kind of
	 constraint solver to make it reasonable (the nodes can't overlap, the angles
	 between edges on a node should endeavour to be equal, that kind of thing).

	 To deal with heterogenous lists we should be able to put switches into the
	 processing that analyse each thing and draw the correct image for it. That's
	 obvious in retrospect, but then what isn't?

	 Let's go back to that dual representation of objects as finished unto
	 themselves, and as affine transformations of normalised objects. That would
	 let us effect the position and orientation of shapes as well as their own
	 properties. But the duality will let you completely ignore the affine aspect
	 unless you want to use it.
** [2017-07-11 Tue 12:20]
	 Interesting connection: we want to be able to create an object from one
	 member of a list and then create a compound by iterating over the list and
	 composing the results. Reminds me of excel. Wait and see if anything useful
	 comes of that.
** [2017-07-17 Mon 11:23]
	 Conceptually the app would be a lot simpler if we didn't store the
	 canvas-wrapper at all, but instead passed the dom node in and created a new
	 one each time we want to interact with it. After all it is supposed to be
	 completely stateless. In principle that would also let us mix and match this
	 stateless canvas API wrapper with traditional uses of canvas.

	 Idea: Explictely define a default style (which is implicitely defined by the
	 canvas init state and probably made explicit in an RFC somewhere) and merge
	 in the style map when drawing. Then restore the default style when
	 finished. This will make my code robust against other people's manipulations
	 and at worst will revert someone else's style to the default. Better than
	 turning everything red, not great, but what more can I do? Well I could query
	 the context for all variables, store and restore them. Is that worth it?

	 I guess the big question is do I want to make this play nice with existing
	 canvas code? That seems like a laudable goal, but more or less irrelevant for
	 my purposes at the moment.
** [2017-07-17 Mon 12:05]
	 Currently working on wrapping the canvas API so that I can treat the canvas
	 element as a window into ℜ². At the same time the API needs to be stateless
	 because I don't want all the global state problems of processing/canvas
	 following me around.

	 It's becoming obvious how tightly I've woven a lot of the drawing and event
	 logic together. It's going to take a fair bit of work to tease everything
	 apart, but that's a good thing. I'm currently getting a bit lost in my own
	 code and there barely is any! That won't do.

** [2017-07-18 Tue 12:11]
	 As it stands I have the window being drawable. I think that's a
	 mistake. Instead shapes should draw themselves to the window. But they need
	 to ask the window first whether they can be drawn and have the window
	 transform the shape into window pixels.

	 So we need something like a RealShape which a window transforms into a
	 PixelShape, which implements Drawable.

	 So the redraw-canvas event side effect would need to take the DOM element,
	 the window state, and the R² state, and pipe it all together to create a
	 PixelShape which then gets drawn.

	 It's not quite that simple because the pixel shape creation has to be
	 recursive and lazy, but it shouldn't be that complicated. Compound
	 PixelShapes would have RealShapes as sub pieces and would go through the
	 window and be converted into PixelShapes that can then be drawn.
** [2017-07-18 Tue 12:29]
	 Or is this all wrong and the shapes should take a window and decide for
	 themselves whether they're in it? After all the window can't see forward to
	 decide on all possible shapes.

	 Only problem there is what clears the frame and draws the grid/axes?
** [2017-07-19 Wed 14:59]
	 So windows know how to transform individual coordinates, but only shapes know
	 what their coordinates are. That seems like a reasonable way to break things
	 down.

	 Got a huge problem at the moment with constructors, everything is broken. Not
	 a good sign when you consider how little is actually going on. Need to
	 simplify. Is the goal to get rid of all the code? Maybe it should be...
** [2017-07-19 Wed 15:06]
	 I've been thinking about lazy rendering, and I have a couple of ideas of how
	 to manage it.

	 The simplest would be to implement instantiation for lazy seqs and insist
	 that anything infinite be lazy. I'm not sure that would scale down to
	 infinite fractals that only show up as you zoom.

	 An alternate approach would be to instantiate on demand within the render
	 loop. This would necessitate passing in both the render schemata and the app
	 state to the render function and would complect rendering with dynamism. Well
	 it wouldn't neccessarily complect if there were a third operator whose only
	 task was to alternately instantiate and render. I don't know. Needs more
	 thought.

** [2017-07-20 Thu 10:36]
	 Current thoughts: It seems it would be best if I implement instantiable
	 properly for lazy seqs and depend on laziness when implementing infinite
	 designs. That is depend on clojure's existing laziness in contrast to
	 implementing my own lazy mutually recursive instantiate-draw-... loop.

	 That's kind of obvious once written down. So why have I been thinking so much
	 about it? I'm still going to have to implement drawing to be lazy, but
	 there's no reason to mix dynamic state with rendering.
** [2017-07-20 Thu 11:14]
	 Should the canvas wrapper invert y-coords? It feels like it would be nicer if
	 the canvas wrapper did nothing but make the api stateless. It would then be
	 up to the window object to invert coords from the normal Cartesian plane to
	 CG coords. It's already responsible for the zoom and pan calculation, so it
	 feels like a more natural place.
** [2017-07-20 Thu 19:44]
	 The app state is thoroghly fuddled.

	 Would it make sense to separate it into 3 distinct pieces; the data, the
	 graphic, and the window? There are actually 2 windows that exist at different
	 points of the lifecycle, call them the design window and the user window. It
	 would be important for the designer to be able to see either, but that's not
	 important right now.

	 Things are still a mess. So how do we structure the moving parts to simplify?

	 Simplification: Unified handling of dom events. That will entail mergeing the
	 current window and input states into a single thing. At first sight that
	 feels like mixing two fundamentally different things, but on further
	 inspection I'm not so sure. Here's the debate: on the one hand, you can think
	 of the graphic as something created by input and existing in the full glory
	 of ℜ², and the window just selects a subset of the plane to render into view.

	 On the other hand, we can think of the graphic as an abstract representation
	 of shape dependent on the history of DOM events. This feels a little awkward,
	 but when you think about it on the user end, moving things around with the
	 mouse and moving the plane by panning and zooming are the same kind of
	 operation. The difference in design between "finishable" constructors and
	 dealing with the reality of a finite view of an infinite thing is superficial
	 because all it is in reality is the fact that we throw away the window state
	 but treat the designer's input state as set in stone.

	 I really need to clarify my writing (and thinking) on the different kinds of
	 graphical dynamism.
** [2017-07-21 Fri 11:00]
	 Gave this some thought last night on the train.

	 Things would be a lot simpler if we have one process that collects raw events
	 into a data structure, a second which reads the first and aggregates higher
	 level events — business logic so to speak; higher level events like strokes
	 made up of hundreds of mouse-moves — and a final reduction of the event
	 stream into the current state of the app.

	 This is, as far as I understand it, Elm's architecture. Time to do some
	 reading.
** [2017-07-22 Sat 11:30]
	 The three type signatures that make up the elm architecture are what I'm
	 going for, but I don't think elm's focus on concurrency is right for this. I
	 want transparency between the data and the end result, actors really muck
	 that up.

	 Parallelism should be hidden behind the scenes if possible, history shows
	 that we humans aren't so hot at dealing with it manually. Conceptually every
	 visual object is concurrent in its dependency on the data. But once we
	 introduce constraints, it won't be so easy. Fortunately the constraints — in
	 so far as I've thought about them — are primarily visual, so they can still
	 be transparent to the designer even if the implementation is horrifyingly
	 complex.
** [2017-08-09 Wed 15:08]
	 Nearly have dragging data to properties working. Two realisations that need
	 to be dealt with before this can be properly considered done:

	 1. Sets of Shapes won't do
			We can't transform the elements inside sets via assoc, or any other
			builtin that I know of. The only options I see immediately are either
			disj, modify, conj which is going to be really awkward, or replace the
			sets with maps. The keys could be the values, or they could be UUIDs, I
			don't know that it matters.

	 2. Instantiation
			If we try to query the shape data by value then we need to pass the
			uninstantiated data into the query builder otherwise it won't match the
			actual app data.
** [2017-08-10 Thu 22:21]
	 Dragging properties into visual objects is working in the most basic way. Now
	 I need some interesting data to try it on.

	 I'm going to need space/time representations of data next or else using the
	 data binding is going to be exceedingly tedious
